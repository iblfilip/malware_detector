from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, \
    AlbertTokenizer, AlbertForSequenceClassification, RobertaForSequenceClassification, RobertaTokenizer, \
    DistilBertForSequenceClassification, DistilBertTokenizer, FlaubertForSequenceClassification, FlaubertTokenizer, \
    LongformerForSequenceClassification, LongformerTokenizer
import torch
import time
import os
import numpy as np
import random
from keras.preprocessing.sequence import pad_sequences
from torch.utils.data import TensorDataset, DataLoader, RandomSampler
from helpers.utils import format_time, print_model_layers
from scipy.special import softmax
from helpers.evaluation import compute_majority_voting, compute_mean_average, print_results
from helpers.plots import show_tokens_length, plot_loss
import logging

logger = logging.getLogger(__name__)


class TransformerModel:
    def __init__(self, config):
        self.config = config
        self.model_path = os.path.join(self.config.get('output_dir'), self.config.get('model_name'))
        self.model = None
        self.tokenizer = None
        self.chunk_size_with_special_tokens = self.config.get('chunk_size') + 2

        self.is_cuda_available = torch.cuda.is_available()
        self.device = None

        if self.config.get('do_training'):
            logger.info('Initializing Transformer model with config')
            self.load_model_and_tokenizer(from_local=False)
        else:
            logger.info('Loading Transformer from ' + self.model_path)
            self.load_model_and_tokenizer(from_local=True)
        self.config.print()

        if self.is_cuda_available:
            self.device = torch.device('cuda')

    def batch_reports(self, reports, labels=[]):
        """
        Splitting reports to batches and tokenizing them with WordPiece tokenizer.
        :param reports: list of reports
        :param labels: list of corresponding labels
        :return:
            input_ids - list of input chunks encoded with tokenizer
            input_labels - labels for input ids
            input_length - number of chunks for each report, for plotting reasons
            input_report_ids - each chunk has its ID based on report to which it belongs
        """
        input_ids = []
        input_labels = []
        input_length = []
        input_report_ids = []

        if not labels:
            report_tuples = reports
        else:
            report_tuples = list(zip(reports, labels))

        for i_report, report_tuple in enumerate(report_tuples):

            if isinstance(report_tuple, tuple):
                report = report_tuple[0]
                label = report_tuple[1]
            else:
                report = report_tuple

            batches = []

            for attr in report:
                if report[attr] is '':
                    continue

                tokenized_attr = self.tokenizer.tokenize(report[attr][:15000])
                token_iter = 0
                i = 0
                token_seq = []

                while i < len(tokenized_attr[:self.config.get('tokens_threshold')]):
                    token = tokenized_attr[i]
                    token_seq.append(token)

                    if token_iter == self.config.get('chunk_size') or i == len(tokenized_attr) - 1:
                        batches.append(token_seq)
                        token_seq = []
                        token_iter = 0
                        if i != len(tokenized_attr) - 1:
                            if i < self.config.get('stride'):
                                i = 0
                            else:
                                i -= self.config.get('stride')

                    token_iter += 1
                    i += 1

            for batch in batches:
                encoded_sent = self.tokenizer.encode(
                    batch,
                    add_special_tokens=True
                )
                input_ids.append(encoded_sent)
                input_report_ids.append(i_report)

                if isinstance(report_tuple, tuple):
                    input_labels.append(label)

            if i_report % 30 == 0:
                logger.info('Report {} split to chunks, # of chunks for report: {}'.format(i_report, len(batches)))

        return input_ids, input_labels, input_length, input_report_ids

    @staticmethod
    def create_attention_mask(input_ids):
        """
        Create attention mask for input inds
        :param input_ids:
        :return: attention mask
        """
        attention_masks = []
        for report in input_ids:
            att_mask = [int(token_id > 0) for token_id in report]
            attention_masks.append(att_mask)
        return attention_masks

    def save(self, args=None):
        """
        Save Transformer model
        """
        if not os.path.exists(self.config.get('output_dir')):
            os.makedirs(self.config.get('output_dir'))

        logger.info('Save model to {}'.format(self.model_path))

        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model
        model_to_save.save_pretrained(self.model_path)
        self.tokenizer.save_pretrained(self.model_path)

        # save training arguments
        if args:
            torch.save(args, os.path.join(self.model_path, 'training_args.bin'))

    def pick_model_and_tokenizer(self):
        """
        Helper function to initialize tokenizer and model based on config
        :return: tokenizer, model instance, name of model
        """
        if self.config.get('model') == 'bert':
            tokenizer = BertTokenizer
            model = BertForSequenceClassification
            model_name = 'bert-base-uncased'
        elif self.config.get('model') == 'albert':
            tokenizer = AlbertTokenizer
            model = AlbertForSequenceClassification
            model_name = 'albert-base-v2'
        elif self.config.get('model') == 'roberta':
            tokenizer = RobertaTokenizer
            model = RobertaForSequenceClassification
            model_name = 'roberta-base'
        elif self.config.get('model') == 'distilbert':
            tokenizer = DistilBertTokenizer
            model = DistilBertForSequenceClassification
            model_name = 'distilbert-base-uncased'
        elif self.config.get('model') == 'flaubert':
            tokenizer = FlaubertTokenizer
            model = FlaubertForSequenceClassification
            model_name = 'flaubert/flaubert_base_cased'
        elif self.config.get('model') == 'longformer':
            tokenizer = LongformerTokenizer
            model = LongformerForSequenceClassification
            model_name = 'allenai/longformer-base-4096'
        return tokenizer, model, model_name

    def load_model_and_tokenizer(self, from_local=False):
        """
        Helper function for loading Transformer model and tokenizer according to configuration
        :param from_local: mark whether use locally saved model and tokenizer
        :return:
        """
        tokenizer, model, model_name = self.pick_model_and_tokenizer()
        if from_local:
            self.tokenizer = tokenizer.from_pretrained(self.model_path, do_lower_case=True)
            self.model = model.from_pretrained(
                self.model_path,
                num_labels=2,
                output_attentions=False,
                output_hidden_states=False
            )
        else:
            self.tokenizer = tokenizer.from_pretrained(model_name, do_lower_case=True)
            self.model = model.from_pretrained(
                model_name,
                num_labels=2,
                output_attentions=False,
                output_hidden_states=False
            )

    def train(self, train_reports, train_labels, validation_reports, validation_labels, save_model=False,
              plot_stats=False):
        """
        Training loop for training Transformer on downstream text classification task
        :param train_reports: dataset of reports for training
        :param train_labels: labels of training report dataset
        :param validation_reports: dataset of reports for validation
        :param validation_labels: labels of validation report dataset
        :param save_model: mark whether save trained model
        """

        # prepare training data
        train_input_ids, train_input_labels, train_input_length, _ = self.batch_reports(train_reports, train_labels)

        train_inputs = pad_sequences(train_input_ids, maxlen=self.chunk_size_with_special_tokens,
                                     dtype='long',
                                     value=0, truncating='post', padding='post')

        train_masks = TransformerModel.create_attention_mask(train_inputs)

        # prepare validation data
        validation_input_ids, validation_input_labels, validation_input_length, validation_report_ids = self.batch_reports(
            validation_reports, validation_labels)

        validation_inputs = pad_sequences(validation_input_ids,
                                          maxlen=self.chunk_size_with_special_tokens,
                                          dtype='long',
                                          value=0, truncating='post', padding='post')

        validation_masks = TransformerModel.create_attention_mask(validation_inputs)

        if plot_stats:
            show_tokens_length(train_input_length, self.config.get('tokens_threshold'))

        # add training data to tensors
        train_inputs = torch.tensor(train_inputs)
        train_input_labels = torch.tensor(train_input_labels)
        train_masks = torch.tensor(train_masks)

        # add validation data to tensors
        validation_inputs = torch.tensor(validation_inputs)
        validation_input_labels = torch.tensor(validation_input_labels)
        validation_masks = torch.tensor(validation_masks)
        validation_report_ids = torch.tensor(validation_report_ids)

        # create dataloader for training set
        train_data = TensorDataset(train_inputs, train_masks, train_input_labels)
        train_sampler = RandomSampler(train_data)
        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=self.config.get('batch_size'))

        # create dataloader for validation set
        validation_data = TensorDataset(validation_inputs, validation_masks, validation_input_labels,
                                        validation_report_ids)
        validation_sampler = RandomSampler(validation_data)
        validation_dataloader = DataLoader(validation_data, sampler=validation_sampler,
                                           batch_size=self.config.get('batch_size'))

        if self.is_cuda_available:
            self.model.cuda()

        print_model_layers(self.model.named_parameters())

        optimizer = AdamW(self.model.parameters(),
                          lr=self.config.get('learning_rate'),
                          eps=self.config.get('eps'))
        epochs = self.config.get('n_epochs')
        total_steps = len(train_dataloader) * epochs
        scheduler = get_linear_schedule_with_warmup(optimizer,
                                                    num_warmup_steps=0,
                                                    num_training_steps=total_steps)

        # training

        # seed to make model reproducible
        seed_val = self.config.get('random_value')
        random.seed(seed_val)
        np.random.seed(seed_val)
        torch.manual_seed(seed_val)
        torch.cuda.manual_seed_all(seed_val)

        loss_values = []

        # training loop
        for epoch_i in range(0, epochs):
            logger.info('---- Epoch: {} / {} ----'.format(epoch_i + 1, epochs))
            logger.info('training')

            t0 = time.time()

            # reset total loss for epoch
            total_loss = 0

            # put model into training mode
            self.model.train()

            for step, batch in enumerate(train_dataloader):
                if step % 100 == 0 and not step == 0:
                    elapsed = format_time(time.time() - t0)

                    logger.info('Batch {:>5,} of {:>5,}. Elapsed {:}'.format(step, len(train_dataloader), elapsed))

                # unpack training batch from dataloader
                b_input_ids = batch[0].to(self.device)
                b_input_mask = batch[1].to(self.device)
                b_labels = batch[2].to(self.device)

                self.model.zero_grad()

                if self.config.get('model') == 'bert':
                    outputs = self.model(b_input_ids,
                                         token_type_ids=None,
                                         attention_mask=b_input_mask,
                                         labels=b_labels)
                else:
                    outputs = self.model(b_input_ids,
                                         attention_mask=b_input_mask,
                                         labels=b_labels)

                loss = outputs[0]

                total_loss += loss.item()

                # backward pass to calculate gradients
                loss.backward()

                # clip gradients to 1.0, help prevent exploding gradients
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)

                optimizer.step()
                scheduler.step()

            # calculate average loss over training data
            avg_train_loss = total_loss / len(train_dataloader)
            loss_values.append(avg_train_loss)

            logger.info(' Average  training  loss: {0:2f}'.format(avg_train_loss))
            logger.info(' Training epoch took {:}'.format(format_time(time.time() - t0)))

            # validation
            logger.info('Validating training loop')

            t0 = time.time()

            # put model into validation mode
            self.model.eval()

            validation_preds = []
            validation_true_labels = []
            validation_report_ids = []

            for batch in validation_dataloader:
                batch = tuple(t.to(self.device) for t in batch)

                b_input_ids, b_input_mask, b_labels, b_report_ids = batch

                with torch.no_grad():
                    if self.config.get('model') == 'bert':
                        outputs = self.model(b_input_ids,
                                             token_type_ids=None,
                                             attention_mask=b_input_mask)
                    else:
                        outputs = self.model(b_input_ids,
                                             attention_mask=b_input_mask)

                # extracting logits and disconnecting from tensors
                logits = outputs[0]
                logits = logits.detach().cpu().numpy()
                label_ids = b_labels.to('cpu').numpy()
                batch_report_ids = b_report_ids.to('cpu').numpy()

                # transform logits to predictions
                predics = softmax(logits)
                validation_preds.append(predics)
                validation_report_ids.append(batch_report_ids)
                validation_true_labels.append(label_ids)

            validation_preds = np.concatenate(validation_preds, axis=0)
            validation_report_ids = np.concatenate(validation_report_ids, axis=0)
            validation_true_labels = np.concatenate(validation_true_labels, axis=0)

            pred_labels = np.argmax(validation_preds, axis=1)

            majority_predictions = compute_majority_voting(validation_report_ids, pred_labels)
            mean_predictions = compute_mean_average(validation_report_ids, validation_preds)

            logger.info('   Majority scoring on validation dataset')
            print_results(majority_predictions, validation_labels)
            logger.info('   Mean scoring on validation dataset')
            print_results(mean_predictions, validation_labels)
            logger.info('   Just scoring on validation dataset')
            print_results(pred_labels, validation_true_labels)

            logger.info('Validation took: {:}'.format(format_time(time.time() - t0)))

        logger.info('Training completed')

        plot_loss(loss_values)

        if save_model:
            self.save()

    def predict(self, test_reports, test_labels):
        """
        Run prediction and evaluation on reports dataset, print results
        :param test_reports: dataset of reports
        :param test_labels: labels of report dataset
        """

        # preparing test data
        test_input_ids, test_input_labels, test_input_length, test_report_ids = self.batch_reports(test_reports,
                                                                                                   test_labels)
        test_inputs = pad_sequences(test_input_ids, maxlen=self.chunk_size_with_special_tokens,
                                    dtype='long',
                                    value=0, truncating='post', padding='post')

        test_masks = TransformerModel.create_attention_mask(test_inputs)

        # connecting test data to tensors
        test_inputs = torch.tensor(test_inputs)
        test_input_labels = torch.tensor(test_input_labels)
        test_masks = torch.tensor(test_masks)
        test_report_ids = torch.tensor(test_report_ids)

        # create dataloader for test set
        test_data = TensorDataset(test_inputs, test_masks, test_input_labels, test_report_ids)
        test_sampler = RandomSampler(test_data)
        test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=self.config.get('batch_size'))

        if self.is_cuda_available:
            self.model.cuda()

        logger.info('Predicting labels for {:,} test reports...'.format(len(test_inputs)))

        self.model.eval()

        test_preds, true_labels, report_ids = [], [], []

        t0 = time.time()

        for (step, batch) in enumerate(test_dataloader):
            if self.is_cuda_available:
                batch = tuple(t.to(self.device) for t in batch)
            else:
                batch = tuple(t for t in batch)

            if step % 100 == 0 and not step == 0:
                elapsed = format_time(time.time() - t0)

                logger.info('Batch {:>5,} of {:>5,}. Elapsed {:}'.format(step, len(test_dataloader), elapsed))

            b_input_ids, b_input_mask, b_labels, b_report_ids = batch

            with torch.no_grad():
                if self.config.get('model') == 'bert':
                    outputs = self.model(b_input_ids,
                                         token_type_ids=None,
                                         attention_mask=b_input_mask)
                else:
                    outputs = self.model(b_input_ids,
                                         attention_mask=b_input_mask)

            logits = outputs[0]

            # extracting logits and disconnecting from tensors
            logits = logits.detach().cpu().numpy()
            label_ids = b_labels.to('cpu').numpy()
            batch_report_ids = b_report_ids.to('cpu').numpy()

            # transform logits to predictions
            batch_preds = softmax(logits)

            test_preds.append(batch_preds)
            true_labels.append(label_ids)
            report_ids.append(batch_report_ids)

        logger.info('Done')

        # combine predictions across batches
        test_preds = np.concatenate(test_preds, axis=0)
        true_labels = np.concatenate(true_labels, axis=0)
        report_ids = np.concatenate(report_ids, axis=0)

        pred_labels = np.argmax(test_preds, axis=1)

        majority_predictions = compute_majority_voting(report_ids, pred_labels)
        mean_predictions = compute_mean_average(report_ids, test_preds)

        # with majority voting
        logger.info('   Majority scoring on test dataset')
        print_results(majority_predictions, test_labels)

        # mean prediction
        logger.info('   Mean scoring on test dataset')
        print_results(mean_predictions, test_labels)

        # just predictions
        logger.info('   Just predics scoring on test dataset')
        print_results(pred_labels, true_labels)

    def predict_report(self, report, pooling_strategy='mean'):
        """
        Classification of single behavioural report
        :param report:
        :return:
        """

        # prepare data
        test_input_ids, _, _, test_report_ids = self.batch_reports([report])

        test_inputs = pad_sequences(test_input_ids, maxlen=self.chunk_size_with_special_tokens,
                                    dtype='long',
                                    value=0, truncating='post', padding='post')

        test_masks = TransformerModel.create_attention_mask(test_inputs)

        # connect data to tensors
        test_inputs = torch.tensor(test_inputs)
        test_masks = torch.tensor(test_masks)
        test_report_ids = torch.tensor(test_report_ids)

        # create dataloader
        test_data = TensorDataset(test_inputs, test_masks, test_report_ids)
        test_sampler = RandomSampler(test_data)
        test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=1)

        if self.is_cuda_available:
            self.model.cuda()

        logger.info('Predicting label for report')

        self.model.eval()

        preds, report_ids = [], []

        t0 = time.time()

        for (step, batch) in enumerate(test_dataloader):
            if self.is_cuda_available:
                batch = tuple(t.to(self.device) for t in batch)
            else:
                batch = tuple(t for t in batch)

            if step % 100 == 0 and not step == 0:
                elapsed = format_time(time.time() - t0)

                logger.info('Batch {:>5,} of {:>5,}. Elapsed {:}'.format(step, len(test_dataloader), elapsed))

            b_input_ids, b_input_mask, b_report_ids = batch

            with torch.no_grad():
                if self.config.get('model') == 'bert':
                    outputs = self.model(b_input_ids,
                                         token_type_ids=None,
                                         attention_mask=b_input_mask)
                else:
                    outputs = self.model(b_input_ids,
                                         attention_mask=b_input_mask)

            logits = outputs[0]

            logits = logits.detach().cpu().numpy()
            batch_report_ids = b_report_ids.to('cpu').numpy()

            batch_preds = softmax(logits)

            preds.append(batch_preds)
            report_ids.append(batch_report_ids)

        # combine predictions across batches
        preds = np.concatenate(preds, axis=0)
        report_ids = np.concatenate(report_ids, axis=0)
        pred_labels = np.argmax(preds, axis=1)

        if pooling_strategy == 'mean':
            prediction = compute_mean_average(report_ids, preds)[0]
        else:
            prediction = compute_majority_voting(report_ids, pred_labels)[0]

        return prediction
