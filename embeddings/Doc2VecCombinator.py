from gensim.models import Doc2Vec
from gensim.test.test_doc2vec import ConcatenatedDoc2Vec
from gensim.models.doc2vec import TaggedDocument
import numpy as np
from nltk.tokenize import RegexpTokenizer
from tqdm import tqdm
import logging

logger = logging.getLogger(__name__)

tqdm.pandas(desc="progress-bar")


class Doc2VecCombinator:
    """
    Doc2Vec class for concatenationg of DM and DBOW models
    """
    def __init__(self, model_dm_path, model_dbow_path):
        """
        Load DM and DBOW trained models, create concatenated model
        :param model_dm_path: path to DM model
        :param model_dbow_path: path to DBOW model
        """
        logger.info('Combining DM and DBOW models')
        self.tokenizer = RegexpTokenizer(r'\w+')
        self.model_dm = Doc2Vec.load(model_dm_path)
        self.model_dbow = Doc2Vec.load(model_dbow_path)
        self.model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)
        self.model_dm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)
        self.concatenated_model = ConcatenatedDoc2Vec([self.model_dbow, self.model_dm])

    def infer_vectors(self, reports):
        """
        Conduct vectorization of reports dataset using concatenated model
        :param reports: reports dataset
        :return: vector representation
        """
        logger.info('Infering vectors from concatenated Doc2Vec model')
        # TODO zkuist dat misto label do tagged documentu index
        # tagged_docs = [TaggedDocument(self.tokenizer.tokenize(report), [label]) for label, report in zip(labels, reports)]
        tagged_docs = [TaggedDocument(self.tokenizer.tokenize(report), [i]) for i, report in enumerate(reports)]
        vecs = [self.concatenated_model.infer_vector(tag.words) for tag in tagged_docs]
        vecs = np.array(vecs)
        return vecs
