import json
import numpy as np
import pandas as pd
from elasticsearch import Elasticsearch
from transformers import TransfoXLTokenizer, TransfoXLModel, TransfoXLConfig
import torch
from simpletransformers.classification import ClassificationModel
from sklearn.model_selection import train_test_split


es = Elasticsearch()


def open_and_clean():
    with open('datasets/reports.json') as file:
        data = json.load(file)
        print('length of reports {}'.format(len(data['reports'])))
        n_malware = 0
        n_benign = 0
        labels = []
        for rep in data['reports']:
            if rep['label'] == 'malware':
                n_malware += 1
                labels.append(1)
            else:
                n_benign += 1
                labels.append(0)

        cleaned_reports = clean_reports(data['reports'])
        return cleaned_reports, labels


def clean_reports(reports):
    print('cleaning reports')
    cleaned_reports = []
    for rep in reports:
        cleaned_words = []
        for word in rep['report'].lower().split(' '):
            word = word.replace(".", " ")
            word = word.replace(",", " ")
            word = word.replace(":", " ")
            word = word.replace("\"", " ")
            word = word.replace("!", " ")
            word = word.replace("â€œ", " ")
            word = word.replace("â€˜", " ")
            word = word.replace("*", " ")
            cleaned_words.append(word)
        cleaned_reports.append(' '.join(cleaned_words))
    return cleaned_reports


def embed_vectors():
    index = 'android-xl-vects'
    reports, labels = open_and_clean()

    list_all = []
    for report, label in zip(reports, labels):
        list_all.append([report, label])

    all_df = pd.DataFrame(list_all)
    print(all_df.head())

    all_df = all_df.sample(frac=1)

    training_df = all_df[:430]
    test_df = all_df[430:]

    print(all_df.head())

    #config = TransfoXLConfig()

    model = ClassificationModel(
        "xlnet", "xlnet-base-cased", use_cuda=False
    )

    model.train_model(training_df)

    result, model_outputs, wrong_predictions = model.eval_model(test_df)

    #tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')
    #model = TransfoXLModel(config)
    #model = TransfoXLModel.from_pretrained('transfo-xl-wt103')

    #inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
    #outputs = model(**inputs)
    #necp = model.get_input_embeddings()
    #last_hidden_states = outputs

    #token_embeddings = torch.stack(last_hidden_states, dim=0)
    #token_embeddings = torch.squeeze(last_hidden_states, dim=1)
    #token_embeddings = token_embeddings.permute(1, 0, 2)

    #token_embeddings = token_embeddings.detach().numpy()
    print('neco')


    #for report, label in zip(reports, labels):


embed_vectors()

