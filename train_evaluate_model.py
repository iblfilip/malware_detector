from config.config import Config
from helpers.utils import load_dataset, split_dataset, print_args, check_args
from helpers.evaluation import print_results, print_confusion_matrix
from embeddings.TransformerModel import TransformerModel
from classifiers.SVC import SVCModel
from classifiers.XGBoost import XGBoostModel
from embeddings.TfidfModel import TfidfModel
from embeddings.Doc2VecModel import Doc2VecModel
from embeddings.Doc2VecCombinator import Doc2VecCombinator
import argparse
import os
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
print = logger.info


class Trainer:
    def __init__(self):
        self.output_dir = 'saved_models'

        # ---> Init params, overwritten by Command Line Args
        self.do_training_emb = False
        self.do_training_cls = False
        self.dataset_path = 'datasets/reports.json'
        self.embedding = 'bert'
        self.classifier = 'xgboost'
        # <---

        if args.train_emb:
            self.do_training_emb = True

        if args.train_cls:
            self.do_training_cls = True

        if args.dataset_path:
            self.dataset_path = args.dataset_path

        if args.embedding:
            self.embedding = args.embedding

        if args.classifier:
            self.classifier = args.classifier

        if args.verbose:
            logging.basicConfig(level=logging.DEBUG)
        else:
            logging.basicConfig(level=logging.INFO)

        check_args(self.__dict__)

        logger.info('Runner initialized with args')
        print_args(self.__dict__)

    def classify(self, model, x_train, y_train, x_validation, y_validation, x_test, y_test):
        """
        Run training and evaluation of classifier model specified as argument, print results
        :param model: initialized classifier
        :param x_train: train dataset
        :param y_train: train labels
        :param x_validation: validation dataset
        :param y_validation: validation labels
        :param x_test: test dataset
        :param y_test: teest labels
        """
        if self.do_training_cls:
            model.train(x_train, y_train, save_model=True)

        train_preds = model.predict(x_train, 'train')
        print_results(train_preds, y_train)
        logger.info('   Confusion matrix on train dataset')
        print_confusion_matrix(train_preds, y_train)

        validation_preds = model.predict(x_validation, 'validation')
        print_results(validation_preds, y_validation)
        logger.info('   Confusion matrix on validation dataset')
        print_confusion_matrix(validation_preds, y_validation)

        test_preds = model.predict(x_test, 'test')
        print_results(test_preds, y_test)
        logger.info('   Confusion matrix on test dataset')
        print_confusion_matrix(test_preds, y_test)

    def doc2vec(self, train_reports, train_labels, validation_reports, validation_labels, test_reports,
                test_labels):
        """
        Run Paragraph Vectors (Doc2Vec) embedding and classification on the outputted vectors
        Model PV-DM and PV-DBOW are assembled, evaluated separately and then concatenated and evaluated
        """

        # DM model initialization
        logger.info('--- DM model classification ---')

        dm_conf = Config({
            'do_training': self.do_training_emb,
            'model_name': 'd2v_dm_model',
            'output_dir': self.output_dir,

            # following params used only for training
            'dm': 1,
            'vector_size': 1000,
            'window': 10,
            'negative': 5,
            'hs': 0,
            'min_count': 50,
            'sample': 0,
            'alpha': 0.025,
            'compute_loss': True,
            'epochs': 10,
            'start_alpha': 0.025,
            'end_alpha': -0.00025,
        })
        dm_model = Doc2VecModel(dm_conf)

        # DM model training
        if self.do_training_emb:
            dm_model.train(train_reports, train_labels, save_model=True)

        # DM model reports embedding
        dm_train_vecs = dm_model.infer_vectors(train_reports, train_labels)
        dm_validation_vecs = dm_model.infer_vectors(validation_reports, validation_labels)
        dm_test_vecs = dm_model.infer_vectors(test_reports, test_labels)

        # classification on DM model's output
        if self.classifier == 'svc':
            svc_conf = Config({
                'do_training': self.do_training_cls,
                'model_name': dm_conf.get('model_name'),
                'output_dir': self.output_dir,

                # following params used only for training
                'C': 1.0,
                'loss': 'hinge',
                'gamma': 'auto',
                'kernel': 'linear',
                'random_value': 42
            })
            model = SVCModel(svc_conf)
            self.classify(model, dm_train_vecs, train_labels, dm_validation_vecs, validation_labels, dm_test_vecs,
                          test_labels)

        elif self.classifier == 'xgboost':
            xgb_conf = Config({
                'do_training': self.do_training_cls,
                'model_name': dm_conf.get('model_name'),
                'output_dir': self.output_dir,
                "n_estimators": [80],

                # following params used only for training
                "eta": [0.01, 0.02],
                "max_depth": [2, 3, 4],
                "learning_rate": [0.05, 0.1, 0.2],
                "min_child_weight": [3,5],
                "gamma": [0.3],
                'colsample_bytree': [0.3],
                'n_jobs': 4,
                'scoring': 'neg_log_loss',
                'cv': 3,
            })
            model = XGBoostModel(xgb_conf)
            self.classify(model, dm_train_vecs, train_labels, dm_validation_vecs, validation_labels, dm_test_vecs,
                          test_labels)

        # DBOW model initialization
        logger.info('--- DBOW model classification ---')

        dbow_conf = Config({
            'do_training': self.do_training_emb,
            'model_name': 'd2v_dbow_model',
            'output_dir': self.output_dir,

            # following params used only for training
            'dm': 0,
            'vector_size': 1000,
            'window': 10,
            'negative': 5,
            'hs': 0,
            'min_count': 25,
            'sample': 0,
            'alpha': 0.015,
            'compute_loss': True,
            'epochs': 10,
            'start_alpha': 0.015,
            'end_alpha': 0.00015
        })

        dbow_model = Doc2VecModel(dbow_conf)

        # DBOW model training
        if self.do_training_emb:
            dbow_model.train(train_reports, train_labels, save_model=True)

        # DBOW model reports embedding
        dbow_train_vecs = dbow_model.infer_vectors(train_reports, train_labels)
        dbow_validation_vecs = dbow_model.infer_vectors(validation_reports, validation_labels)
        dbow_test_vecs = dbow_model.infer_vectors(test_reports, test_labels)

        # classification on DBOW model's output
        if self.classifier == 'svc':
            svc_conf = Config({
                'do_training': self.do_training_cls,
                'model_name': dbow_conf.get('model_name'),
                'output_dir': self.output_dir,

                # following params used only for training
                'C': 1.0,
                'loss': 'hinge',
                'gamma': 'auto',
                'kernel': 'linear',
                'random_value': 42
            })
            model = SVCModel(svc_conf)
            self.classify(model, dbow_train_vecs, train_labels, dbow_validation_vecs, validation_labels, dbow_test_vecs,
                          test_labels)

        elif self.classifier == 'xgboost':
            xgb_conf = Config({
                'do_training': self.do_training_cls,
                'model_name': dbow_conf.get('model_name'),
                'output_dir': self.output_dir,
                "n_estimators": [80],

                # following params used only for training
                "eta": [0.01, 0.02],
                "max_depth": [2, 3, 4],
                "learning_rate": [0.05, 0.1, 0.2],
                "min_child_weight": [3,5],
                "gamma": [0.3],
                'colsample_bytree': [0.3],
                'n_jobs': 4,
                'scoring': 'neg_log_loss',
                'cv': 3,
            })
            model = XGBoostModel(xgb_conf)
            self.classify(model, dbow_train_vecs, train_labels, dbow_validation_vecs, validation_labels, dbow_test_vecs,
                          test_labels)

        # Models concatenation
        logger.info('--- Combined model classification ---')

        concat_model_name = 'd2v_concat'
        combination = Doc2VecCombinator(
            os.path.join(dm_model.model_path, dm_model.config.get('model_name') + '.d2v'),
            os.path.join(dbow_model.model_path, dbow_model.config.get('model_name') + '.d2v')
        )

        # embedding
        conc_train_vecs = combination.infer_vectors(train_reports)
        conc_validation_vecs = combination.infer_vectors(validation_reports)
        conc_test_vecs = combination.infer_vectors(test_reports)

        # classification on concatenated model's output
        if self.classifier == 'svc':
            svc_conf = Config({
                'do_training': self.do_training_cls,
                'model_name': concat_model_name,
                'output_dir': self.output_dir,

                # following params used only for training
                'C': 1.0,
                'loss': 'hinge',
                'gamma': 'auto',
                'kernel': 'linear',
                'random_value': 42
            })
            model = SVCModel(svc_conf)
            self.classify(model, conc_train_vecs, train_labels, conc_validation_vecs, validation_labels, conc_test_vecs,
                          test_labels)

        elif self.classifier == 'xgboost':
            xgb_conf = Config({
                'do_training': self.do_training_cls,
                'model_name': concat_model_name,
                'output_dir': self.output_dir,
                "n_estimators": [80],

                # following params used only for training
                "eta": [0.01, 0.02],
                "max_depth": [2, 3, 4],
                "learning_rate": [0.05, 0.1, 0.2],
                "min_child_weight": [3, 5],
                "gamma": [0.3],
                'colsample_bytree': [0.3],
                'n_jobs': 4,
                'scoring': 'neg_log_loss',
                'cv': 3,
            })
            model = XGBoostModel(xgb_conf)
            self.classify(model, conc_train_vecs, train_labels, conc_validation_vecs, validation_labels, conc_test_vecs,
                          test_labels)

    def tfidf(self, train_reports, train_labels, validation_reports, validation_labels, test_reports,
              test_labels):
        """
        Run TF-IDF model embedding and then classification on outputted vectors
        """
        tfidf_conf = Config({
            'do_training': self.do_training_emb,
            'model_name': 'tfidf_range_1_2_kbest_10000',
            'output_dir': self.output_dir,

            # following params used only for training
            'ngram_range': (1, 2),
            'max_features': 100000,
            'k_best_features': 10000,
            'smooth_idf': True,
            'use_idf': True
        })

        tfidf_model = TfidfModel(tfidf_conf)

        # training TF-IDF model
        if self.do_training_emb:
            tfidf_model.train(train_reports, train_labels, save_model=True)

        # text embedding
        train_vectors = tfidf_model.transform(train_reports)
        validation_vectors = tfidf_model.transform(validation_reports)
        test_vectors = tfidf_model.transform(test_reports)

        # classification
        if self.classifier == 'svc':
            svc_conf = Config({
                'do_training': self.do_training_cls,
                'model_name': tfidf_conf.get('model_name'),
                'output_dir': self.output_dir,

                # following params used only for training
                'C': 1.0,
                'loss': 'hinge',
                'gamma': 'auto',
                'kernel': 'linear',
                'random_value': 42
            })
            model = SVCModel(svc_conf)
            self.classify(model,
                          train_vectors,
                          train_labels,
                          validation_vectors,
                          validation_labels,
                          test_vectors,
                          test_labels)

        elif self.classifier == 'xgboost':
            xgb_conf = Config({
                "do_training": self.do_training_cls,
                'model_name': tfidf_conf.get('model_name'),
                'output_dir': self.output_dir,
                "n_estimators": [80],

                # following params used only for training
                "eta": [0.01, 0.02],
                "max_depth": [2, 3, 4],
                "learning_rate": [0.05, 0.1, 0.2],
                "min_child_weight": [3, 5],
                "gamma": [0.3],
                'colsample_bytree': [0.3],
                'n_jobs': 4,
                'scoring': 'neg_log_loss',
                'cv': 3,
            })
            model = XGBoostModel(xgb_conf)
            self.classify(model, train_vectors, train_labels, validation_vectors, validation_labels, test_vectors,
                          test_labels)

    def transformer(self, train_reports, train_labels, validation_reports, validation_labels, test_reports,
                    test_labels):
        """
        Run Transformer model with classification head
        """

        # ---> specify DistilBERT model's config
        if self.embedding == 'distilbert':
            conf = Config({
                'do_training': self.do_training_emb,
                'model': self.embedding,
                'model_name': 'distilbert_1500_batch8_lr2e-5_4_509',
                'output_dir': self.output_dir,
                'tokens_threshold': 1500,
                'chunk_size': 511,
                'stride': 0,
                'batch_size': 8,

                # following params used only for training
                'learning_rate': 2e-5,
                'eps': 1e-8,
                'n_epochs': 4,
                'random_value': 42,
            })
        # <---

        # ---> specify RoBERTa model's config
        elif self.embedding == 'roberta':
            conf = Config({
                'do_training': self.do_training_emb,
                'model': self.embedding,
                'model_name': 'roberta_3000_batch8_lr1e-5_6_500',
                'output_dir': self.output_dir,
                'tokens_threshold': 3000,
                'chunk_size': 502,
                'stride': 0,
                'batch_size': 8,

                # following params used only for training
                'learning_rate': 1e-5,
                'eps': 1e-8,
                'n_epochs': 6,
                'random_value': 42,
            })
        # <---

        # ---> specify BERT model's config
        elif self.embedding == 'bert':
            conf = Config({
                'do_training': self.do_training_emb,
                'model': self.embedding,
                'model_name': 'bert_3000_batch8_lr1e-5_4',
                'output_dir': self.output_dir,
                'tokens_threshold': 3000,
                'chunk_size': 502,
                'stride': 0,
                'batch_size': 8,

                # following params used only for training
                'learning_rate': 1e-5,
                'eps': 1e-8,
                'n_epochs': 4,
                'random_value': 42,
            })
        # <---

        model = TransformerModel(conf)

        # model's training
        if self.do_training_emb:
            model.train(train_reports, train_labels, validation_reports, validation_labels, save_model=True)

        # classification with model
        model.predict(test_reports, test_labels)

    def run_model(self):
        """
        Load JSON dataset with reports, subsample the dataset and run model based on required config
        """

        # mark whether to keep the attributes in reports or return report content as one big string
        attrs_to_string = True if self.embedding == 'tfidf' or self.embedding == 'doc2vec' else False

        reports, labels = load_dataset(self.dataset_path, attrs_to_string)

        # sub-sampling
        train_reports, train_labels, validation_reports, validation_labels, test_reports, test_labels = split_dataset(
            reports,
            labels)

        reports, labels = None, None

        if self.embedding == 'tfidf':
            self.tfidf(train_reports, train_labels, validation_reports, validation_labels, test_reports,
                       test_labels)

        elif self.embedding == 'doc2vec':
            self.doc2vec(train_reports, train_labels, validation_reports, validation_labels, test_reports,
                         test_labels)

        elif self.embedding == 'bert' \
                or self.embedding == 'albert' \
                or self.embedding == 'roberta' \
                or self.embedding == 'distilbert' \
                or self.embedding == 'flaubert':
            self.transformer(train_reports, train_labels, validation_reports, validation_labels, test_reports,
                             test_labels)
        logger.info('Task completed')


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Running models on behavioural reports')
    parser.add_argument("-TE", "--train_emb", help="mark if to do training of embedding", action="store_true")
    parser.add_argument("-TC", "--train_cls", help="mark if to do training of classifier", action="store_true")
    parser.add_argument("-D", "--dataset_path", help="path to JSON dataset with reports")
    parser.add_argument("-E", "--embedding", help="which embedding method to use -> tfidf | doc2vec | bert")
    parser.add_argument("-C", "--classifier",
                        help="which classifier to use for tfidf or doc2vec embedding -> svc | xgboost")
    parser.add_argument("-v", "--verbose", help="use verbose logging level DEBUG", action="store_true")

    args = parser.parse_args()

    trainer = Trainer()
    trainer.run_model()
