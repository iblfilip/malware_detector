from config.config import Config
from helpers.utils import load_dataset, split_dataset
from helpers.evaluation import print_results
from models.TransformerModel import TransformerModel
from models.LongformerModel import LongformerModel
from longformer_with_simple import Longformer_simple
from classifiers.SVC import SVCModel
from classifiers.XGBoost import XGBoostModel
from models.TfidfModel import TfidfModel
from models.Doc2VecModel import Doc2VecModel
from models.Doc2VecCombinator import Doc2VecCombinator
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class Trainer:
    def __init__(self):
        self.do_training_emb = True
        self.do_training_cls = False
        # self.dataset_name = 'reports_apkinfo.manifest_apkinfo.dex_strings_apkinfo.interesting_strings_apkinfo.files'
        self.dataset_name = 'reports_'
        self.embedding = 'doc2vec'
        self.classifier_name = 'xgboost'
        self.output_dir = 'save_models/'

        # if args.train_emb:
        #     self.do_training_emb = True
        #
        # if args.train_cls:
        #     self.do_training_cls = True
        #
        # if args.dataset:
        #     self.dataset_name = args.dataset
        #
        # if args.embedding:
        #     self.embedding = args.embedding
        #
        # if args.classifier:
        #     self.classifier_name = args.classifier
        #
        # if args.output_dir:
        #     self.output_dir = args.output_dir
        #
        # if args.verbose:
        #     logging.basicConfig(level=logging.DEBUG)
        # else:
        #     logging.basicConfig(level=logging.INFO)

        logger.info('Runner initialized with args')
        # self.print_args()

    def print_args(self):
        logger.info(' ----------------------------')
        logger.info('| training embedding: ' + str(self.do_training_emb))
        logger.info('| training classifiers: ' + str(self.do_training_cls))
        logger.info('| dataset: ' + self.dataset_name)
        logger.info('| embedding: ' + self.embedding)
        logger.info('| classifier: ' + self.classifier_name)
        logger.info('| output dir: ' + self.output_dir)
        logger.info(' ----------------------------')

    def classify(self, model, x_train, y_train, x_validation, y_validation, x_test, y_test):
        if self.do_training_cls:
            model.train(x_train, y_train, save_model=True)

        train_preds = model.predict(x_train, 'train')
        validation_preds = model.predict(x_validation, 'validation')
        test_preds = model.predict(x_test, 'test')

        print_results(train_preds, y_train)
        print_results(validation_preds, y_validation)
        print_results(test_preds, y_test)

    def doc2vec(self, train_reports, train_labels, validation_reports, validation_labels, test_reports,
                test_labels):
        logger.info('--- DM model classification ---')
        dm_conf = Config({
            'do_training': self.do_training_emb,
            'model_name': 'model',
            'output_dir': self.output_dir,
            'dm': 1,
            'vector_size': 1000,
            'window': 10,
            'negative': 5,
            'hs': 0,
            'min_count': 50,
            'sample': 0,
            'alpha': 0.025,
            'compute_loss': True,
            'epochs': 10,
            'start_alpha': 0.025,
            'end_alpha': -0.00025,
        })
        dm_model = Doc2VecModel(dm_conf)

        # if self.do_training_emb:
        #     dm_model.train(train_reports, train_labels, save_model=True)
        #
        # dm_train_vecs = dm_model.infer_vectors(train_reports, train_labels)
        # dm_validation_vecs = dm_model.infer_vectors(validation_reports, validation_labels)
        # dm_test_vecs = dm_model.infer_vectors(test_reports, test_labels)

        # if self.classifier_name == 'svc':
        #     svc_conf = Config({
        #         'do_training': self.do_training_cls,
        #         'model_name': 'svc_d2v_dm',
        #         'output_dir': self.output_dir,
        #         'C': 1.0,
        #         'loss': 'hinge',
        #         'gamma': 'auto',
        #         'kernel': 'linear',
        #         'random_value': 42
        #     })
        #     model = SVCModel(svc_conf)
        #     self.classify(model, dm_train_vecs, train_labels, dm_validation_vecs, validation_labels, dm_test_vecs,
        #                   test_labels)
        #
        # elif self.classifier_name == 'xgboost':
        #     xgb_conf = Config({
        #         'do_training': self.do_training_cls,
        #         'model_name': 'xgb_d2v_dm',
        #         'output_dir': self.output_dir,
        #         "n_estimators": [80],
        #         "eta": [0.01],
        #         "max_depth": [2],
        #         "learning_rate": [0.1],
        #         "min_child_weight": [5],
        #         "gamma": [0.3],
        #         'colsample_bytree': [0.3],
        #         'n_jobs': 4,
        #         'scoring': 'neg_log_loss',
        #         'cv': 3,
        #     })
        #     model = XGBoostModel(xgb_conf)
        #     self.classify(model, dm_train_vecs, train_labels, dm_validation_vecs, validation_labels, dm_test_vecs,
        #                   test_labels)

        logger.info('--- DBOW model classification ---')
        dbow_conf = Config({
            'do_training': self.do_training_emb,
            'model_name': 'model',
            'output_dir': self.output_dir,
            'dm': 0,
            'vector_size': 1000,
            'window': 10,
            'negative': 5,
            'hs': 0,
            'min_count': 50,
            'sample': 0,
            'alpha': 0.025,
            'compute_loss': True,
            'epochs': 10,
            'start_alpha': 0.025,
            'end_alpha': -0.00025,
        })

        dbow_model = Doc2VecModel(dbow_conf)

        if self.do_training_emb:
            dbow_model.train(train_reports, train_labels, save_model=True)

        dbow_train_vecs = dbow_model.infer_vectors(train_reports, train_labels)
        dbow_validation_vecs = dbow_model.infer_vectors(validation_reports, validation_labels)
        dbow_test_vecs = dbow_model.infer_vectors(test_reports, test_labels)

        if self.classifier_name == 'svc':
            svc_conf = Config({
                'do_training': self.do_training_cls,
                'model_name': 'svc_d2v_dbow',
                'output_dir': self.output_dir,
                'C': 1.0,
                'loss': 'hinge',
                'gamma': 'auto',
                'kernel': 'linear',
                'random_value': 42
            })
            model = SVCModel(svc_conf)
            self.classify(model, dbow_train_vecs, train_labels, dbow_validation_vecs, validation_labels, dbow_test_vecs,
                          test_labels)

        elif self.classifier_name == 'xgboost':
            xgb_conf = Config({
                'do_training': self.do_training_cls,
                'model_name': 'xgb_d2v_dbow',
                'output_dir': self.output_dir,
                "n_estimators": [80],
                "eta": [0.01],
                "max_depth": [2],
                "learning_rate": [0.1],
                "min_child_weight": [5],
                "gamma": [0.3],
                'colsample_bytree': [0.3],
                'n_jobs': 4,
                'scoring': 'neg_log_loss',
                'cv': 3,
            })
            model = XGBoostModel(xgb_conf)
            self.classify(model, dbow_train_vecs, train_labels, dbow_validation_vecs, validation_labels, dbow_test_vecs,
                          test_labels)

        logger.info('--- Combined model classification ---')
        combination = Doc2VecCombinator(
            dm_model.model_path,
            dbow_model.model_path
        )
        conc_train_vecs = combination.infer_vectors(train_reports)
        conc_validation_vecs = combination.infer_vectors(validation_reports)
        conc_test_vecs = combination.infer_vectors(test_reports)

        if self.classifier_name == 'svc':
            svc_conf = Config({
                'do_training': self.do_training_cls,
                'model_name': 'svc_d2v_concat',
                'output_dir': self.output_dir,
                'C': 1.0,
                'loss': 'hinge',
                'gamma': 'auto',
                'kernel': 'linear',
                'random_value': 42
            })
            model = SVCModel(svc_conf)
            self.classify(model, conc_train_vecs, train_labels, conc_validation_vecs, validation_labels, conc_test_vecs,
                          test_labels)

        elif self.classifier_name == 'xgboost':
            xgb_conf = Config({
                'do_training': self.do_training_cls,
                'model_name': 'xgb_d2v_concat',
                'output_dir': self.output_dir,
                "n_estimators": [80],
                "eta": [0.01],
                "max_depth": [2],
                "learning_rate": [0.1],
                "min_child_weight": [5],
                "gamma": [0.3],
                'colsample_bytree': [0.3],
                'n_jobs': 4,
                'scoring': 'neg_log_loss',
                'cv': 3,
            })
            model = XGBoostModel(xgb_conf)
            self.classify(model, conc_train_vecs, train_labels, conc_validation_vecs, validation_labels, conc_test_vecs,
                          test_labels)

    def tfidf(self, train_reports, train_labels, validation_reports, validation_labels, test_reports,
              test_labels):
        tfidf_conf = Config({
            'do_training': self.do_training_emb,
            'model_name': 'tfidf_range_1_3_kbest_10000',
            'output_dir': self.output_dir,
            'ngram_range': (1, 3),
            'max_features': 100000,
            'k_best_features': 10000,
            'smooth_idf': True,
            'use_idf': True
        })

        tfidf_model = TfidfModel(tfidf_conf)

        if self.do_training_emb:
            tfidf_model.train(train_reports, train_labels, save_model=True)

        train_vectors = tfidf_model.transform(train_reports)
        validation_vectors = tfidf_model.transform(validation_reports)
        test_vectors = tfidf_model.transform(test_reports)

        if self.classifier_name == 'svc':
            svc_conf = Config({
                'do_training': self.do_training_cls,
                'model_name': 'svc_tfidf',
                'output_dir': self.output_dir,
                'C': 1.0,
                'loss': 'hinge',
                'gamma': 'auto',
                'kernel': 'linear',
                'random_value': 42
            })
            model = SVCModel(svc_conf)
            self.classify(model,
                          train_vectors,
                          train_labels,
                          validation_vectors,
                          validation_labels,
                          test_vectors,
                          test_labels)

        elif self.classifier_name == 'xgboost':
            xgb_conf = Config({
                "do_training": self.do_training_cls,
                'model_name': 'xgb_tfidf',
                'output_dir': self.output_dir,
                "n_estimators": [80],
                "eta": [0.01],
                "max_depth": [2],
                "learning_rate": [0.1],
                "min_child_weight": [5],
                "gamma": [0.3],
                'colsample_bytree': [0.3],
                'n_jobs': 4,
                'scoring': 'neg_log_loss',
                'cv': 3,
            })
            model = XGBoostModel(xgb_conf)
            self.classify(model, train_vectors, train_labels, validation_vectors, validation_labels, test_vectors,
                          test_labels)

    def transformer(self, train_reports, train_labels, validation_reports, validation_labels, test_reports,
                    test_labels):
        conf = Config({
            'do_training': self.do_training_emb,
            'model': 'bert',
            'model_name': 'bert_2000_batch8_lr2e-5_vers_2',
            'output_dir': self.output_dir,
            'tokens_threshold': 1500,
            'chunk_size': 500,
            'stride': 0,
            'batch_size': 8,
            'learning_rate': 2e-5,
            'eps': 1e-8,
            'n_epochs': 5,
            'random_value': 42,
        })

        model = TransformerModel(conf)

        if self.do_training_emb:
            model.train(train_reports, train_labels, validation_reports, validation_labels, save_model=True)

        model.predict(test_reports, test_labels)

    def longformer(self, train_reports, train_labels, validation_reports, validation_labels, test_reports,
                   test_labels):
        model = Longformer_simple()
        model.train(train_reports, train_labels, validation_reports, validation_labels)
        # conf = Config({
        #     'do_training': self.do_training_emb,
        #     'model': 'longformer',
        #     'model_name': 'longformer_2000_batch8_lr2e-5_vers_2',
        #     'output_dir': self.output_dir,
        #     'tokens_threshold': 1500,
        #     'chunk_size': 4094,
        #     'stride': 0,
        #     'batch_size': 8,
        #     'learning_rate': 2e-5,
        #     'eps': 1e-8,
        #     'n_epochs': 5,
        #     'random_value': 42,
        # })
        #
        # model = LongformerModel(conf)
        #
        # if self.do_training_emb:
        #     model.train(train_reports, train_labels, validation_reports, validation_labels, save_model=True)
        #
        # model.predict(test_reports, test_labels)


    def run_model(self):
        to_string = True if self.embedding == 'tfidf' or self.embedding == 'doc2vec' else False

        reports, labels = load_dataset(self.dataset_name, to_string)
        train_reports, train_labels, validation_reports, validation_labels, test_reports, test_labels = split_dataset(
            reports,
            labels)

        if self.embedding == 'tfidf':
            self.tfidf(train_reports, train_labels, validation_reports, validation_labels, test_reports,
                       test_labels)

        elif self.embedding == 'doc2vec':
            self.doc2vec(train_reports, train_labels, validation_reports, validation_labels, test_reports,
                         test_labels)


        elif self.embedding == 'bert' or self.embedding == 'albert' or self.embedding == 'roberta' or self.embedding == 'distilbert' or self.embedding == 'flaubert':
            self.transformer(train_reports, train_labels, validation_reports, validation_labels, test_reports,
                             test_labels)

        elif self.embedding == 'longformer':
            self.longformer(train_reports, train_labels, validation_reports, validation_labels, test_reports,
                            test_labels)


if __name__ == "__main__":
    # parser = argparse.ArgumentParser(description='Running models on behavioural reports')
    # parser.add_argument("-T", "--train_emb", help="mark if to do training of embedding", action="store_true")
    # parser.add_argument("-T", "--train_cls", help="mark if to do training of classifier", action="store_true")
    # parser.add_argument("-D", "--dataset", help="which dataset to use")
    # parser.add_argument("-E", "--embedding", help="which embedding method to use -> tfidf | doc2vec | bert")
    # parser.add_argument("-C", "--classifier",
    #                     help="which classifier to use for tfidf or doc2vec embedding -> svc | xgboost")
    # parser.add_argument("-O", "--output_dir", help="Path to directory where models are stored")
    # parser.add_argument("-v", "--verbose", help="use verbose logging level DEBUG", action="store_true")
    #
    # args = parser.parse_args()

    trainer = Trainer()
    trainer.run_model()
