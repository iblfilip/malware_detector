
from sklearn.decomposition import PCA
from sklearn.manifold import MDS
from sklearn.manifold import TSNE
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.metrics import precision_score, recall_score, accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score
import matplotlib.pyplot as plt
import xgboost as xgb

index = 'android'

def reduce_dimensions(vectors):
    print('reducing dimensionality')
    inst = PCA(n_components=2, random_state=1)
    reduced_data = inst.fit_transform(vectors)
    return reduced_data


def plot(mal_vectors, labels):
    fig, ax = plt.subplots()
    for i, vect in enumerate(mal_vectors):
        if labels[i] == 1:
            ax.scatter(vect[0], vect[1], color='#ff7373')
        else:
            ax.scatter(vect[0], vect[1], color='#9f93bd')
    plt.show()


def svc(X_train, X_test, y_train, y_test):
    print('svc')
    svc_model = LinearSVC(C=1.0, loss='hinge')
    svc_2 = SVC(gamma='auto', kernel='linear')
    clf = make_pipeline(StandardScaler(), svc_2)
    clf.fit(X_train, y_train)
    pred_test = clf.predict(X_test)
    pred_train = clf.predict(X_train)

    print('F1 test {}, F1 train {}'.format(f1_score(y_test, pred_test), f1_score(y_train, pred_train)))
    #print('accuracy test {}, accuracy train {}'.format(accuracy_score(y_test, pred_test), accuracy_score(y_train, pred_train)))


def xgboost(X_train, X_test, y_train, y_test):
    print('XGB')
    parameters = {
        "n_estimators": [80],
        "eta": [0.01],
        "max_depth": [2],
        "learning_rate": [0.1],
        "min_child_weight": [5],
        "gamma": [0.3],
        'colsample_bytree': [0.3]
    }
    # parameters = {
    #     "n_estimators": [80, 100],
    #     "eta": [0.01, 0.02, 0.05],
    #     "max_depth": [2, 3],
    #     "learning_rate": [0.1, 0.3, 0.5],
    #     "min_child_weight": [1, 5, 7],
    #     "gamma": [0.3],
    #     'colsample_bytree': [0.3, 0.4, 0.5, 0.7]
    # }
    steps = 20

    # TODO try apply Stratified kFold
    model = xgb.XGBClassifier()
    grid = GridSearchCV(model,
                        parameters, n_jobs=4,
                        scoring="neg_log_loss",
                        cv=3)
    grid.fit(X_train, y_train)

    best_parameters = grid.best_params_
    n_estims = best_parameters['n_estimators']
    #print(best_parameters)
    best_model = grid.best_estimator_

    preds_train = best_model.predict(X_train, ntree_limit=n_estims)
    preds_test = best_model.predict(X_test, ntree_limit=n_estims)


    print('F1 test {}, F1 train {}'.format(f1_score(y_test, preds_test), f1_score(y_train, preds_train)))
    #print("Accuracy test {}, acc train {}".format(accuracy_score(y_test, preds_test), accuracy_score(y_train, preds_train)))


#get_all()
