import numpy as np
from sklearn.metrics import f1_score, accuracy_score
from table_logger import TableLogger
import logging

logger = logging.getLogger(__name__)


def compute_majority_voting(report_ids, predictions):
    """
    Compute majority voting on predictions
    :param report_ids: list of report ids
    :param predictions: list of predictions corresponding to report ids
    :return: predicted labels
    """
    if len(report_ids) != len(predictions):
        raise Exception('report_ids and predictions arrays must have same length')
        return

    predicted_labels = []

    for id in range(np.amax(report_ids) + 1):
        idxs = np.where(report_ids == id)[0]
        actual_preds = [predictions[idx] for idx in idxs]

        unique, counts = np.unique(actual_preds, return_counts=True)

        if len(counts) > 1:
            if counts[0] > counts[1]:
                predicted_labels.append(0)
            else:
                predicted_labels.append(1)
        else:
            predicted_labels.append(unique[0])

    return predicted_labels


def compute_mean_average(report_ids, predictions):
    """
    Compute mean average on predictions
    :param report_ids: list of report ids
    :param predictions: list of predictions corresponding to report ids
    :return: predicted labels
    """
    if len(report_ids) != len(predictions):
        raise Exception('report_ids and predictions arrays must have same length')
        return

    predicted_labels = []
    report_ids = np.array(report_ids)

    for id in range(np.amax(report_ids) + 1):
        idxs = np.where(report_ids == id)[0]
        actual_preds = [predictions[idx] for idx in idxs]
        mean_preds = np.mean(actual_preds, axis=0)
        pred = np.argmax(mean_preds)
        predicted_labels.append(pred)

    return predicted_labels


def print_results(preds, y):
    tbl = TableLogger(columns='Metric,Score', float_format='{:,.3f}'.format, default_colwidth=15)
    tbl('Accuracy', accuracy_score(y, preds))
    tbl('F1', f1_score(y, preds))
