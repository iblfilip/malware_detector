import numpy as np
import json
import re
import os
import sys
import pickle
import datetime
from sklearn.model_selection import train_test_split
from table_logger import TableLogger
import logging

logger = logging.getLogger(__name__)


def load_dataset(dataset_path, attrs_to_string=False):
    """
    Load JSON dataset from /datasets directory
    :param dataset_path: path to dataset file
    :param attrs_to_string: mark, whether to return report's content as one big string or keep attributes there
    :return: reports - list of reports, labels - list of corresponding report's labels
    """
    logger.info('Loading dataset from ' + dataset_path)
    with open(dataset_path) as file:
        data = json.load(file)
        reports, labels = prepare_reports(data, attrs_to_string)
        return reports, labels


def load_report(file_path):
    with open(file_path) as file:
        data = json.load(file)
        return data


def prepare_reports(reports, attrs_to_string=False):
    bare_reports = []
    labels = []
    for rep in reports:
        if attrs_to_string:
            bare = convert_to_string(rep['report'])
            bare_reports.append(bare)
        else:
            bare_reports.append(rep['report'])

        if rep['label'] == 'malware':
            labels.append(1)
        else:
            labels.append(0)

    _, counts = np.unique(labels, return_counts=True)
    logger.info('   Dataset statistics')
    tbl = TableLogger(columns='Class,# of reports')
    tbl('malware', counts[1])
    tbl('benignware', counts[0])
    print('+----------------------+----------------------+')
    return bare_reports, labels


def convert_to_string(report):
    return re.sub('[{}":]', '', json.dumps(report))


def clean_report(report):
    """
    Conduct report cleeaning, removing special characters, replacing only numerical tokens with NUMBER token
    :param report: behavioral report
    :return: cleaned report
    """
    for attr in report:
        attr_str = json.dumps(report[attr])
        tokens = attr_str.lower().split(' ')
        cleaned_tokens = []
        for word in tokens:
            if word.isnumeric():
                word = 'number'
            word = word.replace(".", " ")
            word = word.replace(",", " ")
            word = word.replace(":", " ")
            word = word.replace(";", " ")
            word = word.replace("\"", " ")
            word = word.replace("/", " ")
            word = word.replace("_", " ")
            word = re.sub('[^a-zA-Z0-9 \n.]', '', word)

            word = word.rstrip()
            word = word.lstrip()

            if len(word) > 1:
                cleaned_tokens.append(word)

        report[attr] = ' '.join(cleaned_tokens)
    return report


def split_dataset(reports, labels):
    """
    Subsample list of reports to train, validation and test datasets
    :param reports: list of reports
    :param labels: list of corresponding labels
    :return: train_reports, train_labels, validation_reports, validation_labels, test_reports, test_labels
    """
    train_val_reports, test_reports, train_val_labels, test_labels = train_test_split(reports, labels,
                                                                                      test_size=0.165,
                                                                                      random_state=42)
    train_reports, validation_reports, train_labels, validation_labels = train_test_split(train_val_reports,
                                                                                          train_val_labels,
                                                                                          test_size=0.2,
                                                                                          random_state=42)

    return train_reports, train_labels, validation_reports, validation_labels, test_reports, test_labels


def check_path(path):
    exists_path = os.path.exists(path)
    if not exists_path:
        logger.info('Model output dir does not exist. Creating {}'.format(path))
        os.mkdir(path)


def format_time(elapsed):
    "return string hh:mm:ss"
    elapsed_rounded = int(round(elapsed))
    return str(datetime.timedelta(seconds=elapsed_rounded))


def print_args(obj):
    tbl = TableLogger(columns='Parameter,Value')
    [tbl(attr, value) for attr, value in obj.items()]
    print('+----------------------+----------------------+')


def print_model_layers(params):
    params = list(params)
    print('Bert has {} different named params'.format(len(params)))

    print('--- Embedding layer ---')
    for p in params[0:5]:
        print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))

    print('--- First transformer --- ')
    for p in params[5:21]:
        print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))

    print('--- Output layer --- ')
    for p in params[-4:]:
        print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))


def save(model_path, model):
    path = model_path + '.pkl'
    with open(path, 'wb') as file:
        pickle.dump(model, file)
        logger.info('Model saved to ' + model_path)


def load(model_path):
    # TODO check if exists model
    path = model_path + '.pkl'
    exists_path = os.path.exists(path)
    if not exists_path:
        logger.error('Path not found, {}'.format(path))
        sys.exit(1)

    logger.info('Loading model from ' + path)
    with open(path, 'rb') as file:
        return pickle.load(file)
