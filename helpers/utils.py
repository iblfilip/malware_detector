import numpy as np
import json
import re
import string
import pickle
import datetime
from sklearn.model_selection import train_test_split
import logging

logger = logging.getLogger(__name__)


def load_dataset(file_name='reports', to_string=False):
    folder = 'datasets/'
    dataset_path = folder + file_name + '.json'
    logger.info('>>> Using dataset ' + dataset_path)
    with open(dataset_path) as file:
        data = json.load(file)
        reports, labels = prepare_reports(data, to_string)
        return reports, labels


def prepare_reports(reports, to_string=False):
    bare_reports = []
    labels = []
    for rep in reports:
        if to_string:
            bare = re.sub('[{}":]', '', json.dumps(rep['report']))
            bare_reports.append(bare)
        else:
            bare_reports.append(rep['report'])

        if rep['label'] == 'malware':
            labels.append(1)
        else:
            labels.append(0)

    _, counts = np.unique(labels, return_counts=True)
    logger.info('   Dataset statistics')
    logger.info(' --------------------- ')
    logger.info('| # of reports    {} |'.format(len(reports)))
    logger.info('| # of malware    {} |'.format(counts[1]))
    logger.info('| # of beningware {} |'.format(counts[0]))
    logger.info(' --------------------- ')
    return bare_reports, labels


def clean_report(report):
    final_len = 0
    init_len = 0
    for attr in report['report']:
        attr_str = json.dumps(report['report'][attr])
        init_len += len(attr_str)
        tokens = attr_str.lower().split(' ')
        cleaned_tokens = []
        for word in tokens:
            if word.isnumeric():
                word = 'number'
            word = word.replace(".", " ")
            word = word.replace(",", " ")
            word = word.replace(":", " ")
            word = word.replace(";", " ")
            word = word.replace("\"", " ")
            word = word.replace("/", " ")
            word = word.replace("_", " ")
            word = re.sub('[^a-zA-Z0-9 \n.]', '', word)

            word = word.rstrip()
            word = word.lstrip()

            if len(word) > 1:
                cleaned_tokens.append(word)

        report['report'][attr] = ' '.join(cleaned_tokens)
        final_len += len(report['report'][attr])
    print('cut by {}, > {} chars, {}'.format(100 - (final_len/init_len) * 100, final_len,  init_len))
    return report


def split_dataset(reports, labels):
    train_val_reports, test_reports, train_val_labels, test_labels = train_test_split(reports, labels,
                                                                                      test_size=0.165,
                                                                                      random_state=42)
    train_reports, validation_reports, train_labels, validation_labels = train_test_split(train_val_reports,
                                                                                          train_val_labels,
                                                                                          test_size=0.2,
                                                                                          random_state=42)

    return train_reports, train_labels, validation_reports, validation_labels, test_reports, test_labels


def format_time(elapsed):
    'return string hh:mm:ss'
    elapsed_rounded = int(round(elapsed))
    return str(datetime.timedelta(seconds=elapsed_rounded))


def print_model_layers(params):
    params = list(params)
    print('Bert has {} different named params'.format(len(params)))

    print('--- Embedding layer ---')
    for p in params[0:5]:
        print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))

    print('--- First transformer --- ')
    for p in params[5:21]:
        print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))

    print('--- Output layer --- ')
    for p in params[-4:]:
        print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))


def save(model_path, model):
    logger.info('Saving model to ' + model_path)
    with open(model_path + '.pkl', 'wb') as file:
        pickle.dump(model, file)


def load(model_path):
    # TODO check if exists model
    logger.info('Loading model from ' + model_path)
    with open(model_path + '.pkl', 'rb') as file:
        return pickle.load(file)