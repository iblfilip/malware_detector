from gensim.models import Doc2Vec
from gensim.test.test_doc2vec import ConcatenatedDoc2Vec
from gensim.models.doc2vec import TaggedDocument
import numpy as np
from nltk.tokenize import RegexpTokenizer
from tqdm import tqdm
import logging

logger = logging.getLogger(__name__)

tqdm.pandas(desc="progress-bar")


class Doc2VecCombinator:
    def __init__(self, model_dm_path, model_dbow_path):
        logger.info('Combining DM and DBOW models')
        self.tokenizer = RegexpTokenizer(r'\w+')
        self.model_dm = Doc2Vec.load(model_dm_path)
        self.model_dbow = Doc2Vec.load(model_dbow_path)
        self.model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)
        self.model_dm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)
        self.concatenated_model = ConcatenatedDoc2Vec([self.model_dbow, self.model_dm])

    def infer_vectors(self, reports, labels):
        logger.info('Infering vectors from concatenated Doc2Vec model')
        tagged_docs = [TaggedDocument(self.tokenizer.tokenize(report), [label]) for label, report in zip(labels, reports)]
        vecs = [self.concatenated_model.infer_vector(tag.words) for tag in tagged_docs]
        vecs = np.array(vecs)
        return vecs
